1-10: Að©ðšðœð¡ðž Sð©ðšð«ð¤

1. What is Spark and why is it preferred over MapReduce?
2. Explain the difference between transformation and action in Spark.
3. How does Spark handle fault tolerance?
4. What is the significance of caching in Spark?
5. Explain the concept of broadcast variables in Spark.
6. What is the role of Spark SQL in data processing?
7. How does Spark handle memory management?
8. Discuss the significance of partitioning in Spark.
9. Explain the difference between RDDs, DataFrames, and Datasets.
10. What are the different deployment modes available in Spark?

11-20: ð‡ðƒð…ð’

11. What is HDFS and how does it differ from traditional file systems?
12. Explain the architecture of HDFS.
13. How does HDFS ensure fault tolerance?
14. What is the default block size in HDFS?
15. How does HDFS handle data replication?
16. Describe the process of reading and writing data in HDFS.
17. What is the role of the Namenode and Datanode in HDFS?
18. How is data integrity maintained in HDFS?
19. Discuss the significance of block placement in HDFS.
20. How can you troubleshoot issues in HDFS?

21-30 ð‡ð¢ð¯ðž

21. What is Hive and how does it relate to Hadoop ecosystem?
22. Explain the difference between external and managed tables in Hive.
23. How does Hive optimize queries?
24. What is the metastore in Hive and why is it important?
25. What are Hive partitions and how are they useful?
26. Discuss the role of HiveQL in querying data.
27. How does Hive support ACID transactions?
28. Explain the process of data ingestion in Hive.
29. How can you improve Hive query performance?
30. What are the best practices for managing Hive tables?

31-40 ð€ð¢ð«ðŸð¥ð¨ð°

31. What is Apache Airflow and why is it used?
32. Explain the key components of Airflow.
33. How does Airflow handle task dependencies?
34. What is a DAG in Airflow?
35. Discuss the role of the Airflow scheduler.
36. How does Airflow support dynamic workflows?
37. What are Airflow operators and how are they used?
38. How can you monitor and troubleshoot workflows in Airflow?
39. Explain the concept of XComs in Airflow.
40. How can you scale Airflow for larger workflows?


Recent DATA Engineer interview experience.
Round-1
Q)How do you handle nulls in data?(df.na.drop() or df.dropna() or use when and otherwise to assign any value:df=df.withColumn("a",when(col("b").isnull(),"M")))
Q)How to read files recursively in a folder in pyspark?(use recursivelookup option or use *.file_type at folder level)
Q)When to use partitioning and when to use bucketing?(when partitioning of data creates small file problem then go for bucketing.eg: partition data on age(1-100) will generate 100 small partition files ,instead do bucketing of data in 3 buckets(1-33, 34-67,68-100)).
Q)Query to generate the 2nd highest salary and to remove dups.
Q) delta table and the transaction log and its time travel properties were asked.(restore delta table, print older version of delta table etc.)
Q) optimisation techniques used in project
Round-2
Q)Asked about the basic working of various window functions like lead,lag,dense_rank() etc.
Q)Generate cumulative sum over salary column of a table per department.
Q) Questions on Dimension tables vs fact tables and about star schema.
Q)General idea of Implementation of scd-2.(sql code must have 3 columns like the flag, start_date and end_date to denote each record whether active or not and from when till when as per scd-2)
Q)Count number of occurrences of 'p' in column Fruits as given below.
Example:
Fruit
------- 
apple-------------> 2
pineapple -------> 3
(with cte as(select len(fruit) as l1, len(replace('fruit','p','')) as l2 from t)
select (l1-l2) as p_count from cte)
Q)0,1,1,2,3,5,8-> write function to return fibbonacci series.(used recursion like : return fibb(n-1)+fibb(n-2))
Q)Find the customer who missed atleast two due dates using pyspark.
CusId  Duedate     PaymentDate
C1 01/01/2019   12/30/2018 
C1 01/02/2019 01/25/2019 
C1   01/02/2019    01/24/2019 
C2 05/01/2019 06/01/2019
C2 05/02/2019 02/02/2019
C2 05/03/2019 07/03/2019
**take it as a challenge to solve and u can provide queries in comments below.
Round-3
Q) SQL problem,DDL statements and solution provided in first comment of this post.
Q)1 situation based question to equally distribute water between 3 people with some conditions.
Q)reading a csv file without headers and writing it into parquet?
Q)avro vs parquet vs orc
Q)Driver vs worker nodes how will they behave when increasing memory/cores aggressively.
Q)questions on Compression algo working internally for parquet and internal working and serilization and de-serialization of data.
Q)How did u handles data skewness?
1.calculate total raw data size:%sh du -sch /dbfs:"source path"
2.calculate total partitions needed by dividing data size by 128 mb(140 * 1024/128) 
3.to get total existing partitions in source:df.rdd.getnumpartitions()
4.to check skewness:df.withcolumn("c",spark_partition_id()).groupby("c").count()
compare step 2 and 3,if total partitions in step-2>step-3 use repartition(as we need to increase the number of existing partitions)else use coalesce()).